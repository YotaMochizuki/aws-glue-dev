import os

from modules.etl import transform
from modules.session import create_spark_session


def log_environment_variables():
    """ETLå‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹."""
    print("ğŸ“Œ ä½¿ç”¨ã•ã‚Œã‚‹ç’°å¢ƒå¤‰æ•°:")
    print(f"  AWS_REGION = {os.getenv('AWS_REGION', 'ap-northeast-1')}")
    print(f"  S3_TABLES_BUCKET = {os.getenv('S3_TABLES_BUCKET', '(æœªè¨­å®š)')}")
    print(f"  S3_TABLES_CATALOG_NAME = {os.getenv('S3_TABLES_CATALOG_NAME', '(æœªè¨­å®š)')}")
    print(f"  S3_TABLES_NAME_SPACE = {os.getenv('S3_TABLES_NAME_SPACE', '(æœªè¨­å®š)')}")
    print(f"  S3_TTABLES_TABLE_NAME = {os.getenv('S3_TTABLES_TABLE_NAME', '(æœªè¨­å®š)')}")
    print(f"  JOB_NAME = {os.getenv('JOB_NAME', '(æœªè¨­å®š)')}")
    print("-" * 60)


def run_etl():
    """ETL ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°.

    Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã€ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã€Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚“ã ã‚ã¨çµæœã‚’è¡¨ç¤ºã™ã‚‹.
    """
    spark = create_spark_session()

    # ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿
    data = [(1, "Alice", 30), (2, "Bob", 25), (3, "Charlie", 35)]
    df = spark.createDataFrame(data, ["id", "name", "age"])
    transformed_df = transform(df)

    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
    catalog = os.getenv("S3_TABLES_CATALOG_NAME", "s3tablesbucket")
    namespace = os.getenv("S3_TABLES_NAME_SPACE", "default")
    table = os.getenv("S3_TTABLES_TABLE_NAME", "sample_etl_table")
    table_name = f"{catalog}.{namespace}.{table}"

    # Iceberg ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ä½œæˆ
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {catalog}.{namespace}")

    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    print(f"ğŸ“¦ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ« `{table_name}` ã‚’ä½œæˆä¸­...")
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id INT,
            name STRING,
            age INT,
            name_upper STRING
        )
        USING iceberg
    """)

    # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
    print(f"ğŸ“¤ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ« `{table_name}` ã«æ›¸ãè¾¼ã¿ä¸­...")
    transformed_df.writeTo(table_name).append()

    # çµæœè¡¨ç¤º
    print("ğŸ“„ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰èª­ã¿å–ã‚Š:")
    spark.read.table(table_name).show()


if __name__ == "__main__":
    log_environment_variables()
    run_etl()
