# AWS Glue ジョブ 開発手順

## 前提
`環境構築手順.md`の内容に従い環境構築が完了していること.

---

## 概要
サンプル AWS Glue Job の開発通して開発手順を学ぶ.

---

## サンプル AWS Glue Job の開発

### 1. `launch.json`への追記
下記のように`.vscode/launch.json`へ`main.py`のデバッグ実行用設定を追記する.
```json
{
    "configurations": [
        {
            "name": "Python: Run sample.py",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/sample.py",
            "console": "integratedTerminal",
            "envFile": "${workspaceFolder}/.env_sample"
        },
        //下記を追記(上記設定は環境構築が完了が確認後は不要なので、関連ファイルごと消してもよい)
        {
            "name": "Python: Run main.py", // 構成名（デバッグの選択肢に表示される）
            "type": "debugpy", // デバッガの種類. Python の場合は debugpy を使用
            "request": "launch",// 起動方法. launch はローカルでスクリプトを直接起動するモード
            "program": "${workspaceFolder}/main.py", // デバッグ実行する際のエントリースクリプトファイル
            "console": "integratedTerminal", // 出力先のターミナル（VSCode統合ターミナルを使用）
            "envFile": "${workspaceFolder}/.env" // 環境変数をファイル（.env）を指定.
        },
    ]
}
```

### 2. `.env`へ環境変数を記載
下記`.env`ファイルを追加する.
```env
# 環境変数例
AWS_REGION=ap-northeast-1
S3_TABLES_BUCKET=arn:aws:s3tables:ap-northeast-1:123456789012:bucket/example-data-bucket
S3_TABLES_CATALOG_NAME=s3tablesbucket
S3_TABLES_NAME_SPACE=default
S3_TTABLES_TABLE_NAME=test_table
JOB_NAME=glue-main-job
```

## 3. モジュールの実装
下記のような2つのモジュールを実装.

**modules/etl.py**
```python
# modules/etl.py
from pyspark.sql import DataFrame
from pyspark.sql.functions import upper


def transform(df: DataFrame) -> DataFrame:
    """ETL処理: 名前を大文字に変換し、追加カラムを付与する."""
    return df.withColumn("name_upper", upper(df["name"]))

```

**modules/session.py**
```python
# modules/session.py
import os

from pyspark.sql import SparkSession


def create_spark_session() -> SparkSession:
    """SparkSession を構築する."""
    catalog = os.getenv("S3_TABLES_CATALOG_NAME", "s3tablesbucket")
    return (
        SparkSession.builder.appName(os.getenv("JOB_NAME", "GlueJob"))
        .config("spark.jars", "/jars/s3-tables-catalog-for-iceberg-runtime-0.1.4.jar")
        .config(f"spark.sql.catalog.{catalog}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{catalog}.catalog-impl", "software.amazon.s3tables.iceberg.S3TablesCatalog")
        .config(f"spark.sql.catalog.{catalog}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config(f"spark.sql.catalog.{catalog}.warehouse", os.getenv("S3_TABLES_BUCKET", ""))
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .config("spark.hadoop.aws.region", os.getenv("AWS_REGION", "ap-northeast-1"))
        .getOrCreate()
    )
```



## 4.テストコード作成
**tests/conftest.py**: テスト実行時に使う共通の準備や後片付けを管理するためのファイル
```python
# tests/conftest.py
import pytest
from pyspark.sql import SparkSession


@pytest.fixture(scope="session")
def spark():
    """Pytest セッション全体で共有される SparkSession を生成するフィクスチャ.

    - この fixture は `scope="session"` を指定しており、
      テストセッション中に 1 回だけ SparkSession を生成して再利用します.
    - Spark はローカルモード (local[*]) で全コアを使って起動されます.
    - appName は Spark UI やログで確認できるアプリ名です.
    """
    return SparkSession.builder.master("local[*]").appName("TestSession").getOrCreate()


@pytest.fixture
def sample_data(spark):
    """テスト用のサンプル DataFrame を提供するフィクスチャ.

    - `spark` fixture を使って単一レコードの DataFrame を生成します.
    - カラムは `id` と `name` を持ち、値は `(1, "Alice")` です.
    """
    return spark.createDataFrame([(1, "Alice")], ["id", "name"])


def pytest_sessionstart(session):
    """Pytest のテスト実行前に自動的に呼び出される特殊なフック関数.

    - テストの開始時にコンソールにメッセージを出力します.
    - ログ確認や CI/CD における可視性を高めるために便利です.
    """
    print("🚀 テスト開始！")

```

**tests/test_etl.py**: `modules/etl.py`のテストコードファイル
```python
# tests/test_etl.py
from modules.etl import transform


def test_transform_adds_uppercase_column(sample_data):
    """Transform 関数が name_upper カラムを追加するかをテストする."""
    # fixture から受け取ったサンプル DataFrame に対して変換処理を実行
    result_df = transform(sample_data)

    # 結果を取得し、name_upper カラムが期待通りか検証
    result = result_df.collect()[0]
    assert result["name_upper"] == "ALICE"

```

**tests/test_session.py**: `modules/session.py`のテストコードファイル
```python
from modules.session import create_spark_session


def test_create_spark_session():
    """create_spark_session() が None ではなく、SparkSession を正しく生成するかを確認します.

    appName が空ではなく設定されているかを確認します.
    """
    spark = create_spark_session()
    assert spark is not None
    assert spark.sparkContext.appName.startswith("GlueJob") or spark.sparkContext.appName != ""

```

---

## 5. テストコード実行準備
`.vscode/settings.json`にテストコードを登録
```json
{
    "python.defaultInterpreterPath": "/usr/bin/python",
    "python.testing.pytestEnabled": true,
    "python.testing.unittestEnabled": false,
    "python.testing.pytestArgs": [
        "tests.py",// サンプルコードのテストコード(不要なので消してよい)
        "tests" // テストコードが含まれるフォルダ(追記分)
    ],
    "[python]": {
        "editor.codeActionsOnSave": {
            "source.fixAll.ruff": "explicit",
            "source.organizeImports.ruff": "explicit"
        },
        "editor.defaultFormatter": "charliermarsh.ruff",
        "editor.formatOnSave": true,
        "editor.formatOnType": true,
    },
    "autoDocstring.docstringFormat": "google",
    "editor.formatOnType": true,
    "editor.formatOnSave": true
}
```

## 6. テストコード実行
1. `ctrl + P`でコマンドパレットを開く
2. `Test: Run All Tests` を選択


## 7. `main.py`のコーディング
**main.py**
```python
# main.py
import os

from modules.etl import transform
from modules.session import create_spark_session


def log_environment_variables():
    """ETL処理で使用する環境変数をログ出力する."""
    print("📌 使用される環境変数:")
    print(f"  AWS_REGION = {os.getenv('AWS_REGION', 'ap-northeast-1')}")
    print(f"  S3_TABLES_BUCKET = {os.getenv('S3_TABLES_BUCKET', '(未設定)')}")
    print(f"  S3_TABLES_CATALOG_NAME = {os.getenv('S3_TABLES_CATALOG_NAME', '(未設定)')}")
    print(f"  S3_TABLES_NAME_SPACE = {os.getenv('S3_TABLES_NAME_SPACE', '(未設定)')}")
    print(f"  S3_TTABLES_TABLE_NAME = {os.getenv('S3_TTABLES_TABLE_NAME', '(未設定)')}")
    print(f"  JOB_NAME = {os.getenv('JOB_NAME', '(未設定)')}")
    print("-" * 60)


def run_etl():
    """ETL ジョブを実行するメイン関数.

    Spark セッションを作成し、デモデータを変換し、Iceberg テーブルに書き込んだあと結果を表示する.
    """
    spark = create_spark_session()

    # デモデータ
    data = [(1, "Alice", 30), (2, "Bob", 25), (3, "Charlie", 35)]
    df = spark.createDataFrame(data, ["id", "name", "age"])
    transformed_df = transform(df)

    # テーブル情報
    catalog = os.getenv("S3_TABLES_CATALOG_NAME", "s3tablesbucket")
    namespace = os.getenv("S3_TABLES_NAME_SPACE", "default")
    table = os.getenv("S3_TTABLES_TABLE_NAME", "sample_etl_table")
    table_name = f"{catalog}.{namespace}.{table}"

    # Iceberg ネームスペース作成
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {catalog}.{namespace}")

    # テーブル作成
    print(f"📦 Iceberg テーブル `{table_name}` を作成中...")
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id INT,
            name STRING,
            age INT,
            name_upper STRING
        )
        USING iceberg
    """)

    # データ書き込み
    print(f"📤 Iceberg テーブル `{table_name}` に書き込み中...")
    transformed_df.writeTo(table_name).append()

    # 結果表示
    print("📄 Iceberg テーブルから読み取り:")
    spark.read.table(table_name).show()


if __name__ == "__main__":
    log_environment_variables()
    run_etl()


```

## 8. サンプル AWS Glue Job のデバッグ実行
1. `ctrl + P`でコマンドパレットを開く
2. `Debug: Select and Start Debugging` を選択
3. `Python: Run main.py`を選択
