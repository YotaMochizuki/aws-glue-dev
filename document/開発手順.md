# AWS Glue ã‚¸ãƒ§ãƒ– é–‹ç™ºæ‰‹é †

## å‰æ
`ç’°å¢ƒæ§‹ç¯‰æ‰‹é †.md`ã®å†…å®¹ã«å¾“ã„ç’°å¢ƒæ§‹ç¯‰ãŒå®Œäº†ã—ã¦ã„ã‚‹ã“ã¨.

---

## æ¦‚è¦
ã‚µãƒ³ãƒ—ãƒ« AWS Glue Job ã®é–‹ç™ºé€šã—ã¦é–‹ç™ºæ‰‹é †ã‚’å­¦ã¶.

---

## ã‚µãƒ³ãƒ—ãƒ« AWS Glue Job ã®é–‹ç™º

### 1. `launch.json`ã¸ã®è¿½è¨˜
ä¸‹è¨˜ã®ã‚ˆã†ã«`.vscode/launch.json`ã¸`main.py`ã®ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œç”¨è¨­å®šã‚’è¿½è¨˜ã™ã‚‹.
```json
{
    "configurations": [
        {
            "name": "Python: Run sample.py",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/sample.py",
            "console": "integratedTerminal",
            "envFile": "${workspaceFolder}/.env_sample"
        },
        //ä¸‹è¨˜ã‚’è¿½è¨˜(ä¸Šè¨˜è¨­å®šã¯ç’°å¢ƒæ§‹ç¯‰ãŒå®Œäº†ãŒç¢ºèªå¾Œã¯ä¸è¦ãªã®ã§ã€é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨æ¶ˆã—ã¦ã‚‚ã‚ˆã„)
        {
            "name": "Python: Run main.py", // æ§‹æˆåï¼ˆãƒ‡ãƒãƒƒã‚°ã®é¸æŠè‚¢ã«è¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
            "type": "debugpy", // ãƒ‡ãƒãƒƒã‚¬ã®ç¨®é¡. Python ã®å ´åˆã¯ debugpy ã‚’ä½¿ç”¨
            "request": "launch",// èµ·å‹•æ–¹æ³•. launch ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç›´æ¥èµ·å‹•ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰
            "program": "${workspaceFolder}/main.py", // ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œã™ã‚‹éš›ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
            "console": "integratedTerminal", // å‡ºåŠ›å…ˆã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ï¼ˆVSCodeçµ±åˆã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’ä½¿ç”¨ï¼‰
            "envFile": "${workspaceFolder}/.env" // ç’°å¢ƒå¤‰æ•°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.envï¼‰ã‚’æŒ‡å®š.
        },
    ]
}
```

### 2. `.env`ã¸ç’°å¢ƒå¤‰æ•°ã‚’è¨˜è¼‰
ä¸‹è¨˜`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã™ã‚‹.
```env
# ç’°å¢ƒå¤‰æ•°ä¾‹
AWS_REGION=ap-northeast-1
S3_TABLES_BUCKET=arn:aws:s3tables:ap-northeast-1:123456789012:bucket/example-data-bucket
S3_TABLES_CATALOG_NAME=s3tablesbucket
S3_TABLES_NAME_SPACE=default
S3_TTABLES_TABLE_NAME=test_table
JOB_NAME=glue-main-job
```

## 3. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Ÿè£…
ä¸‹è¨˜ã®ã‚ˆã†ãª2ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè£….

**modules/etl.py**
```python
# modules/etl.py
from pyspark.sql import DataFrame
from pyspark.sql.functions import upper


def transform(df: DataFrame) -> DataFrame:
    """ETLå‡¦ç†: åå‰ã‚’å¤§æ–‡å­—ã«å¤‰æ›ã—ã€è¿½åŠ ã‚«ãƒ©ãƒ ã‚’ä»˜ä¸ã™ã‚‹."""
    return df.withColumn("name_upper", upper(df["name"]))

```

**modules/session.py**
```python
# modules/session.py
import os

from pyspark.sql import SparkSession


def create_spark_session() -> SparkSession:
    """SparkSession ã‚’æ§‹ç¯‰ã™ã‚‹."""
    catalog = os.getenv("S3_TABLES_CATALOG_NAME", "s3tablesbucket")
    return (
        SparkSession.builder.appName(os.getenv("JOB_NAME", "GlueJob"))
        .config("spark.jars", "/jars/s3-tables-catalog-for-iceberg-runtime-0.1.4.jar")
        .config(f"spark.sql.catalog.{catalog}", "org.apache.iceberg.spark.SparkCatalog")
        .config(f"spark.sql.catalog.{catalog}.catalog-impl", "software.amazon.s3tables.iceberg.S3TablesCatalog")
        .config(f"spark.sql.catalog.{catalog}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
        .config(f"spark.sql.catalog.{catalog}.warehouse", os.getenv("S3_TABLES_BUCKET", ""))
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
        .config("spark.hadoop.aws.region", os.getenv("AWS_REGION", "ap-northeast-1"))
        .getOrCreate()
    )
```



## 4.ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ä½œæˆ
**tests/conftest.py**: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã«ä½¿ã†å…±é€šã®æº–å‚™ã‚„å¾Œç‰‡ä»˜ã‘ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ•ã‚¡ã‚¤ãƒ«
```python
# tests/conftest.py
import pytest
from pyspark.sql import SparkSession


@pytest.fixture(scope="session")
def spark():
    """Pytest ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã§å…±æœ‰ã•ã‚Œã‚‹ SparkSession ã‚’ç”Ÿæˆã™ã‚‹ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£.

    - ã“ã® fixture ã¯ `scope="session"` ã‚’æŒ‡å®šã—ã¦ãŠã‚Šã€
      ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã« 1 å›ã ã‘ SparkSession ã‚’ç”Ÿæˆã—ã¦å†åˆ©ç”¨ã—ã¾ã™.
    - Spark ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ (local[*]) ã§å…¨ã‚³ã‚¢ã‚’ä½¿ã£ã¦èµ·å‹•ã•ã‚Œã¾ã™.
    - appName ã¯ Spark UI ã‚„ãƒ­ã‚°ã§ç¢ºèªã§ãã‚‹ã‚¢ãƒ—ãƒªåã§ã™.
    """
    return SparkSession.builder.master("local[*]").appName("TestSession").getOrCreate()


@pytest.fixture
def sample_data(spark):
    """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ« DataFrame ã‚’æä¾›ã™ã‚‹ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£.

    - `spark` fixture ã‚’ä½¿ã£ã¦å˜ä¸€ãƒ¬ã‚³ãƒ¼ãƒ‰ã® DataFrame ã‚’ç”Ÿæˆã—ã¾ã™.
    - ã‚«ãƒ©ãƒ ã¯ `id` ã¨ `name` ã‚’æŒã¡ã€å€¤ã¯ `(1, "Alice")` ã§ã™.
    """
    return spark.createDataFrame([(1, "Alice")], ["id", "name"])


def pytest_sessionstart(session):
    """Pytest ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå‰ã«è‡ªå‹•çš„ã«å‘¼ã³å‡ºã•ã‚Œã‚‹ç‰¹æ®Šãªãƒ•ãƒƒã‚¯é–¢æ•°.

    - ãƒ†ã‚¹ãƒˆã®é–‹å§‹æ™‚ã«ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºåŠ›ã—ã¾ã™.
    - ãƒ­ã‚°ç¢ºèªã‚„ CI/CD ã«ãŠã‘ã‚‹å¯è¦–æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ä¾¿åˆ©ã§ã™.
    """
    print("ğŸš€ ãƒ†ã‚¹ãƒˆé–‹å§‹ï¼")

```

**tests/test_etl.py**: `modules/etl.py`ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
```python
# tests/test_etl.py
from modules.etl import transform


def test_transform_adds_uppercase_column(sample_data):
    """Transform é–¢æ•°ãŒ name_upper ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã™ã‚‹ã‹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹."""
    # fixture ã‹ã‚‰å—ã‘å–ã£ãŸã‚µãƒ³ãƒ—ãƒ« DataFrame ã«å¯¾ã—ã¦å¤‰æ›å‡¦ç†ã‚’å®Ÿè¡Œ
    result_df = transform(sample_data)

    # çµæœã‚’å–å¾—ã—ã€name_upper ã‚«ãƒ©ãƒ ãŒæœŸå¾…é€šã‚Šã‹æ¤œè¨¼
    result = result_df.collect()[0]
    assert result["name_upper"] == "ALICE"

```

**tests/test_session.py**: `modules/session.py`ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«
```python
from modules.session import create_spark_session


def test_create_spark_session():
    """create_spark_session() ãŒ None ã§ã¯ãªãã€SparkSession ã‚’æ­£ã—ãç”Ÿæˆã™ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™.

    appName ãŒç©ºã§ã¯ãªãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™.
    """
    spark = create_spark_session()
    assert spark is not None
    assert spark.sparkContext.appName.startswith("GlueJob") or spark.sparkContext.appName != ""

```

---

## 5. ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰å®Ÿè¡Œæº–å‚™
`.vscode/settings.json`ã«ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã‚’ç™»éŒ²
```json
{
    "python.defaultInterpreterPath": "/usr/bin/python",
    "python.testing.pytestEnabled": true,
    "python.testing.unittestEnabled": false,
    "python.testing.pytestArgs": [
        "tests.py",// ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰(ä¸è¦ãªã®ã§æ¶ˆã—ã¦ã‚ˆã„)
        "tests" // ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ãƒ•ã‚©ãƒ«ãƒ€(è¿½è¨˜åˆ†)
    ],
    "[python]": {
        "editor.codeActionsOnSave": {
            "source.fixAll.ruff": "explicit",
            "source.organizeImports.ruff": "explicit"
        },
        "editor.defaultFormatter": "charliermarsh.ruff",
        "editor.formatOnSave": true,
        "editor.formatOnType": true,
    },
    "autoDocstring.docstringFormat": "google",
    "editor.formatOnType": true,
    "editor.formatOnSave": true
}
```

## 6. ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
1. `ctrl + P`ã§ã‚³ãƒãƒ³ãƒ‰ãƒ‘ãƒ¬ãƒƒãƒˆã‚’é–‹ã
2. `Test: Run All Tests` ã‚’é¸æŠ


## 7. `main.py`ã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
**main.py**
```python
# main.py
import os

from modules.etl import transform
from modules.session import create_spark_session


def log_environment_variables():
    """ETLå‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ç’°å¢ƒå¤‰æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹."""
    print("ğŸ“Œ ä½¿ç”¨ã•ã‚Œã‚‹ç’°å¢ƒå¤‰æ•°:")
    print(f"  AWS_REGION = {os.getenv('AWS_REGION', 'ap-northeast-1')}")
    print(f"  S3_TABLES_BUCKET = {os.getenv('S3_TABLES_BUCKET', '(æœªè¨­å®š)')}")
    print(f"  S3_TABLES_CATALOG_NAME = {os.getenv('S3_TABLES_CATALOG_NAME', '(æœªè¨­å®š)')}")
    print(f"  S3_TABLES_NAME_SPACE = {os.getenv('S3_TABLES_NAME_SPACE', '(æœªè¨­å®š)')}")
    print(f"  S3_TTABLES_TABLE_NAME = {os.getenv('S3_TTABLES_TABLE_NAME', '(æœªè¨­å®š)')}")
    print(f"  JOB_NAME = {os.getenv('JOB_NAME', '(æœªè¨­å®š)')}")
    print("-" * 60)


def run_etl():
    """ETL ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°.

    Spark ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã€ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã€Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚“ã ã‚ã¨çµæœã‚’è¡¨ç¤ºã™ã‚‹.
    """
    spark = create_spark_session()

    # ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿
    data = [(1, "Alice", 30), (2, "Bob", 25), (3, "Charlie", 35)]
    df = spark.createDataFrame(data, ["id", "name", "age"])
    transformed_df = transform(df)

    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
    catalog = os.getenv("S3_TABLES_CATALOG_NAME", "s3tablesbucket")
    namespace = os.getenv("S3_TABLES_NAME_SPACE", "default")
    table = os.getenv("S3_TTABLES_TABLE_NAME", "sample_etl_table")
    table_name = f"{catalog}.{namespace}.{table}"

    # Iceberg ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ä½œæˆ
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {catalog}.{namespace}")

    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    print(f"ğŸ“¦ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ« `{table_name}` ã‚’ä½œæˆä¸­...")
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id INT,
            name STRING,
            age INT,
            name_upper STRING
        )
        USING iceberg
    """)

    # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
    print(f"ğŸ“¤ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ« `{table_name}` ã«æ›¸ãè¾¼ã¿ä¸­...")
    transformed_df.writeTo(table_name).append()

    # çµæœè¡¨ç¤º
    print("ğŸ“„ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰èª­ã¿å–ã‚Š:")
    spark.read.table(table_name).show()


if __name__ == "__main__":
    log_environment_variables()
    run_etl()


```

## 8. ã‚µãƒ³ãƒ—ãƒ« AWS Glue Job ã®ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
1. `ctrl + P`ã§ã‚³ãƒãƒ³ãƒ‰ãƒ‘ãƒ¬ãƒƒãƒˆã‚’é–‹ã
2. `Debug: Select and Start Debugging` ã‚’é¸æŠ
3. `Python: Run main.py`ã‚’é¸æŠ
